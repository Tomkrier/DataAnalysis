# 实践实验4：软件集成与改进

## 实验目的

1. 掌握复杂应用程序的开发方法
2. 掌握故障诊断的方法
3. 掌握将UI设计、软件模型结合的能力

## 实验任务

1. 结合LLM、本地数据（招聘数据集）完成基于自然语言交互和可视化图像的就业咨询的完整APP
2. 包含左侧栏（内容自定义，可以是key、参数、约束条件等等）
3. 包含聊天界面（内容包括图文等，根据能力完成，至少必须完成文字交互）
4. 至少包含3个问题（含图文）的交互展示，可以更多（加分）
5. 加分项：部署在服务器上，使用手机打开APP操作
6. 加分项：如果能力可及，可以采用语音输入的方式获取用户提问。
7. 本次没有固定的实验报告格式，请同学们内容格式自拟（.md），包括界面介绍、实现的功能截图。

## 注意事项

### 尽量使用数据驱动，而非逻辑驱动

随着项目进展，软件结构越来越复杂。学生在能力可及的范围内，可以考虑使用数据驱动。

许多的回调操作基础流程是差不多的，虽然使用if-elif-elif....可以将这些操作区分开，但是更好的方式是将逻辑差不多的操作整合成一个统一的处理函数，通过输入列表来实现调用。如果缺乏编程经验，还是需要先完成功能，再考虑优化。

### 学会管理复杂度

在本次实验中，使用streamlit来启动程序，这个方式的问题在于无法使用IDE的跟踪工具去跟踪故障。对于复杂的应用开发，要学会管理复杂度。

例如编写一个test.py程序，对某个问题或函数进行单独测试。例如下面的代码test.py中，test_asso这些开关打开（设置为True)
时就可以针对某个特定函数做单元测试。当测试成功后，再挪到app.py中调用。

```python

cacu_aprior = False
test_asso = False
test_func = False
chat_test = False
print(dataset.info())

if cacu_aprior:
   apriorifile = "apriori.bin"
   skill_list = getSkillList(dataset, 'skill_list')
   big, biga = calcu_apriori(skill_list, 0.01, savefile=apriorifile)
   s = getAsso(biga, 'linux', 0.1)
   print(s)

if test_func:
   # 根据岗位关键词搜索技能
   jobstr = '嵌入式'  # 'C/C++开发工程师'
   prompt = "嵌入式岗位需要什么技能？"
   word, pdict = cacu_postion_skill_wordcount(dataset, jobstr, prompt, count=20)
   print(f"{jobstr} 相关岗位最需要的技能是：{word}")
   print(pdict)
```

### 单步跟踪

以pycharm为例：

1. 点击要设置断点的代码左侧，出现一个红点，如果要取消，点击红点就可以了：

   ![image-20240510105315775](images\7.png)

2. 点击右键选择debug....

   ![image-20240510105436461](images\8.png)

3. 开始跟踪，程序运行到断点处会停止。此时可以观察下方的窗口，可以看到各个变量的值。也可以点击评估器（蓝色圈），输入表达式查看结果。

   ![image-20240510105611879](images\9.png)

4. 如果选择进入函数跟踪，使用F7,如果不进入函数，按F8，如果要去下一个断点，按F9

### 必须掌握辅助编程插件

强烈建议使用辅助编程插件而不是网页版的大模型对话。

推荐的插件：通义千问、coploit等，除了生成代码，还可以就上下文情况做提问。可以帮助快速找到bug。

只需要给出批注，自动补充：

![image-20240510110047917](images\10.png)

可以针对某个上下文位置提问：

![image-20240510110229539](images\11.png)

可以针对运行错误提问：

![image-20240510110401210](images\12.png)

## 参考技术

### 构建聊天界面

参考代码chat_glm.py（先运行这个例子测试一下，注意运行时请输入glm的apikey）

1. 左侧栏

```python
# 这个部分是左边栏目的内容，之前实验1做过，demo中的内容可以替换为之前的内容
with st.sidebar:
   略
```

2. 记录对话历史信息：

` st.session_state`是一种记录在st会话中的变量的方法。在st中的变量都是以字典形式出现的。以变量名：值的形式给出。例如下面这段话的意思是，在session中添加一个叫做message的变量。

```python
# 初始化的对话。如果会话中没有message这个变量，就添加一个。这里添加的就是模拟LLM机器人的口气开启的第一声招呼。
if "messages" not in st.session_state.keys():
   st.session_state.messages = [{"role": "assistant", "content": "你好我是ChatGLM，有什么可以帮助你的？"}]

```

随着会话的进展，消息每一次都被添加到`st.session_state.messages`
中，因此这里，当界面上的插件有变动的时候，都会将历史对话在界面上重写一次。也就是说，时刻刷新历史对话信息到界面上。这里`st.chat_message`
根据角色呈现聊天机器人的头像。st.write是将聊天文本写到界面上。

在`with st.chat_message(message["role"]): `下面段落的内容都会绘制在聊天对话中。

```python
for message in st.session_state.messages:
   with st.chat_message(message["role"]):
      st.write(message["content"])
```

这段代码效果如下：

![image-20240509155847261](images\1.png)

这里还提供了一个清空历史记录的功能。随着时间的增加，对话记录会越来越多。如果你选择每次都考虑历史对话信息，发往LLM的对话token消耗速度会越来越快。（例如用户在前文透露了一些个人信息，那么后面的对话就可以根据前面的历史信息给出用户更加偏爱的回答。这种就是带着历史记录的对话。如果不希望带历史记录，就比如之前实验做的那样，那就不需要考虑历史信息放入chat）

这里将清空历史记录放在左侧栏中（st.sidebar)

```
def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "你好我是ChatGLM，有什么可以帮助你的？"}]

st.sidebar.button('清空聊天记录', on_click=clear_chat_history)
```

**![image-20240509160202531](images\image-20240509160202531.png)**

2. 根据用户输入的对话，调用LLM获取返回信息。将返回信息添加到message中。其中` st.chat_input()`对应下面的输入框：

   ![image-20240509160600397](images\2.png)

```python
# 当用户输入了key之后，才可以显示对话信息。
if len(key) > 1:
   # 界面上绘制输入框，并且将用户输入的内容添加到对话列表中
   if prompt := st.chat_input():
      # 将用户输入的文字（提示词）添加到message中。
      st.session_state.messages.append({"role": "user", "content": prompt})
      with st.chat_message("user"):
         # 将用户输入的提示词以markdown格式打印出来。
         st.markdown(prompt)

      with st.chat_message("assistant"):
         with st.spinner("请求中..."):
            # 调用大模型，将历史对话（st.session_state.messages）送给大模型，返回值放入full_response中。
            full_response =
            ask_glm(key, model, max_tokens, temperature, st.session_state.messages)['choices'][0]['message']['content']
            # 将返回值打印出来。注意，这里还在st.chat_message("assistant")的应答范围内。
            st.markdown(full_response)

            # 将LLM响应的内容也作为对话历史的一部分
            message = {"role": "assistant", "content": full_response}
            st.session_state.messages.append(message)
```

### 在对话中调用LLM

在当前的应用中，通过获得用户的提问，调用LLM转为对本地函数的调用，函数调用的结果再调用LLM转为自然语言回复给大模型。

具体流程如下：

1. 用户输入信息，例如"请问北京薪资最高的岗位是什么？"（提示词）
2.
将提示词信息连同定义了回调函数的tools字段的信息发送给LLM，由LLM判断应该调用哪个函数。例如应该调用的函数是get_max_salary_job()
，假设返回的是”算法工程师“。
3.
重新构造提示词（这里只是举例，具体的自定义）：`用户的问题是"请问北京薪资最高的岗位是什么？"，根据2023年拉勾网数据查询结果是“算法工程师”，请将查询结果转为对用户的回答。`
4. LLM返回润色后的自然语言，例如：`根据2023年拉勾网的查询结果，北京薪资最高的岗位是算法工程师。`
5. 将LLM返回的自然语言作为full_response返回（上面代码中ask_glm的返回，注意这里只是例子，大家可以根据自己的代码情况设计。）

### 在对话中绘制Echart图像

为了更有说服力，需要将本地查询结果呈现出来。为了更好地呈现查询结果，可以在执行本地查询的时候生成图片。

有两种实现方式：

1. 在回调函数中生成图片文件，在应答的时候插入图片。（这个方法简单，如何编写自己查询）

2. 在回调函数中增加数据返回，在界面上调用Echart绘图（其实也可以直接在回调函数中直接渲染）

   效果如下：

![image-20240510095021300](images\3.png)

![image-20240510103226586](images\5.png)

![image-20240510104111384](images\6.png)

例如将Echart的生成结果放入对话中。

### 表格呈现

对于一些问题的结果，也可以考虑使用表格做呈现：（使用`st.dataframe(rdata)`,其中rdata是过滤后的dataframe，注意控制表格长度）

![image-20240510102954173](images\4.png)

## 注意事项

本次实验无需像例子中那样每次聊天都携带历史对话。学生可以选择携带历史记录，也可以不带。考虑到token的消耗量较大，调试多了会涉及费用问题，本次实验推荐使用后者（不带历史对话），且这种方式实现与之前的实验一致，相对简单。

开发过程中一定要注意将前期的功能测试完整，才能继续做后面的实验，不然到了集成阶段会陷入越来越混乱的场面。
